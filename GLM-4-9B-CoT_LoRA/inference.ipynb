{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Annotated, Union\n",
    "\n",
    "import typer\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast\n",
    ")\n",
    "\n",
    "ModelType = Union[PreTrainedModel, PeftModelForCausalLM]\n",
    "TokenizerType = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "        model_dir: Union[str, Path], trust_remote_code: bool = True\n",
    ") -> tuple[ModelType, TokenizerType]:\n",
    "    model_dir = Path(model_dir).expanduser().resolve()\n",
    "    if (model_dir / 'adapter_config.json').exists():\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model.peft_config['default'].base_model_name_or_path\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dir, trust_remote_code=trust_remote_code, device_map='auto'\n",
    "        )\n",
    "        tokenizer_dir = model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_dir, trust_remote_code=trust_remote_code, encode_special_tokens=True, use_fast=False\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2655e802430a4297b64a332fafc96cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../../output/checkpoint-1000\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tools.common_utils import highlight_diff, read_jsonl\n",
    "\n",
    "json_file_path = 'data/dev.jsonl'\n",
    "data_list = read_jsonl(json_file_path)\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 55.3 ms, total: 11.1 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "index = 999\n",
    "row = df.loc[index]\n",
    "single_user_row = row[\"messages\"][0]\n",
    "single_assis_row = row[\"messages\"][1]\n",
    "\n",
    "messages = [single_user_row]\n",
    "output = single_assis_row[\"content\"]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "generate_kwargs = {\n",
    "            \"input_ids\": inputs,\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.8,\n",
    "            \"temperature\": 0.8,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"eos_token_id\": model.config.eos_token_id,\n",
    "        }\n",
    "outputs = model.generate(**generate_kwargs)\n",
    "response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<output>\n",
      "    思考过程：\n",
      "        首先可以确定这是一个股票查询类任务，我们可以将该问题分解为以下几个步骤：\n",
      "    \n",
      "    1. 确定股票名称，query中的股票标准名应该是“捷昌驱动”；\n",
      "    2. 将股票名称转化为股票代码，调用 股票查询-代码(api_0) 获取“捷昌驱动”的股票代码；\n",
      "    3. 根据获取到的股票代码查询该股票的总市值，调用 股票查询-总市值(api_1) 获取“捷昌驱动”的总市值；\n",
      "    4. 同样根据获取到的股票代码查询该股票的静态市盈率，调用 股票查询-静态市盈率(api_2) 获取“捷昌驱动”的静态市盈率；\n",
      "    5. 最后输出总市值和静态市盈率，即 api_1 和 api_2 的结果。\n",
      "    于是最终标准的json格式结果为:\n",
      "        {\"relevant APIs\": [{\"api_id\": \"0\", \"api_name\": \"代码\", \"required_parameters\": [[\"捷昌驱动\"]], \"rely_apis\": [], \"tool_name\": \"股票查询\"}, {\"api_id\": \"1\", \"api_name\": \"总市值\", \"required_parameters\": [\"api_0的结果\"], \"rely_apis\": [\"0\"], \"tool_name\": \"股票查询\"}, {\"api_id\": \"2\", \"api_name\": \"静态市盈率\", \"required_parameters\": [\"api_0的结果\"], \"rely_apis\": [\"0\"], \"tool_name\": \"股票查询\"}], \"result\": [\"api_1的结果\", \"api_2的结果\"]}\n",
      "</output>\n",
      "============================================================\n",
      "<output>\n",
      "    思考过程：\n",
      "        1. 首先，根据问题描述和提供的标准名列表，可以确定这是一个股票查询类任务。问题中提到的产品标准名是“捷昌驱动”，这是一个股票名称。\n",
      "        2. 为了查询股票的相关信息，首先需要将股票名称转换为股票代码。因此，第一步是调用股票查询类中的“代码”API（api_0），输入股票名称“捷昌驱动”，获取对应的股票代码。\n",
      "        3. 获取股票代码后，接下来需要查询该股票的总市值和静态市盈率。这两个指标都属于股票查询类中的API。\n",
      "        4. 调用股票查询类中的“总市值”API（api_1），输入上一步获取的股票代码（即api_0的结果），查询该股票的总市值。\n",
      "        5. 同样地，调用股票查询类中的“静态市盈率”API（api_2），输入上一步获取的股票代码（即api_0的结果），查询该股票的静态市盈率。\n",
      "        6. 最终，输出查询到的总市值和静态市盈率，即api_1的结果和api_2的结果。\n",
      "    \n",
      "    通过以上步骤，可以逐步得到问题的答案，即捷昌驱动的总市值和静态市盈率。\n",
      "    于是最终标准的json格式结果为:\n",
      "        {\"relevant APIs\": [{\"api_id\": \"0\", \"api_name\": \"代码\", \"required_parameters\": [[\"捷昌驱动\"]], \"rely_apis\": [], \"tool_name\": \"股票查询\"}, {\"api_id\": \"1\", \"api_name\": \"总市值\", \"required_parameters\": [\"api_0的结果\"], \"rely_apis\": [\"0\"], \"tool_name\": \"股票查询\"}, {\"api_id\": \"2\", \"api_name\": \"静态市盈率\", \"required_parameters\": [\"api_0的结果\"], \"rely_apis\": [\"0\"], \"tool_name\": \"股票查询\"}], \"result\": [\"api_1的结果\", \"api_2的结果\"]}\n",
      "</output>\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print('='*60)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.standard_name_utils import optimize_parameters\n",
    "data_stock = pd.read_excel('raw_data/标准名.xlsx',sheet_name='股票标准名')\n",
    "data_fund = pd.read_excel('raw_data/标准名.xlsx',sheet_name='基金标准名')\n",
    "\n",
    "fund_standard_name = data_fund['标准基金名称'].to_list()\n",
    "stock_standard_name = data_stock['标准股票名称'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "total_eval_count = 0.0\n",
    "correct_count = 0.0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if random() <= 0.9:\n",
    "        continue\n",
    "    \n",
    "    single_user_row = row[\"messages\"][0]\n",
    "    single_assis_row = row[\"messages\"][1]\n",
    "    \n",
    "    messages = [single_user_row]\n",
    "    output = single_assis_row[\"content\"]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "    \n",
    "    generate_kwargs = {\n",
    "            \"input_ids\": inputs,\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.8,\n",
    "            \"temperature\": 0.8,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"eos_token_id\": model.config.eos_token_id,\n",
    "        }\n",
    "    \n",
    "    outputs = model.generate(**generate_kwargs)\n",
    "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()\n",
    "    \n",
    "    optimized_resp = optimize_parameters(response, fund_standard_name, stock_standard_name)\n",
    "    \n",
    "    total_eval_count += 1\n",
    "    if response == output:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print(\"-----data index-----\")\n",
    "        print(index)\n",
    "        print(\"-----query input-----\")\n",
    "        print(input)\n",
    "        print(\"-----output diff-----\")\n",
    "        print(highlight_diff(output, response))\n",
    "        print(response)\n",
    "        print()\n",
    "    \n",
    "    if total_eval_count == 20:\n",
    "        break\n",
    "    \n",
    "print(\"预测正确的比例：\" + f\"{correct_count / total_eval_count :.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eval_count = 0\n",
    "\n",
    "with open('data/submit.txt','w', encoding=\"utf-8\") as n:\n",
    "    for index, row in test_df.iterrows():  \n",
    "        single_user_row = row[\"messages\"][0]\n",
    "        single_assis_row = row[\"messages\"][1]\n",
    "        \n",
    "        messages = [single_user_row]\n",
    "        output = single_assis_row[\"content\"]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "        \n",
    "        generate_kwargs = {\n",
    "                \"input_ids\": inputs,\n",
    "                \"max_new_tokens\": 1024,\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.8,\n",
    "                \"temperature\": 0.8,\n",
    "                \"repetition_penalty\": 1.2,\n",
    "                \"eos_token_id\": model.config.eos_token_id,\n",
    "            }\n",
    "        \n",
    "        outputs = model.generate(**generate_kwargs)\n",
    "        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()\n",
    "        \n",
    "        optimized_resp = optimize_parameters(response, fund_standard_name, stock_standard_name)        \n",
    "        total_eval_count += 1\n",
    "        \n",
    "        n.write(optimized_resp+'\\n')\n",
    "        \n",
    "        if total_eval_count % 10 == 0:\n",
    "            print(\"现在是第\" + f\"{total_eval_count}\" + \"条数据\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
